{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c6cc3d8-8da5-42c5-b055-e8fe3a47b2d9",
   "metadata": {},
   "source": [
    "## <a name=\"0\">Fine-tuning an LLM on Amazon SageMaker</a>\n",
    "    \n",
    "In this solution, you explore how to fine-tune a pre-trained large languages model (LLM), which is a powerful technique in the field of generative artificial intelligence (AI). LLMs are pre-trained on enormous amounts of data, making them highly effective in grasping the nuances of language and generating coherent responses. These models have learned to extract useful features and patterns from the data, making them a valuable resource for various machine learning (ML) tasks.\n",
    "\n",
    "With fine-tuning, also known as transfer learning, we can use the knowledge gained by a pre-trained model and apply it to a different but related task. Instead of training a model from scratch, we start with a pre-trained model and modify it to adapt to our specific problem domain. This approach saves significant computational resources, and it benefits from the generalization capabilities of the pre-trained model.\n",
    "\n",
    "This notebook walks you through the step-by-step process of fine-tuning a pre-trained model. It covers the following primary steps:\n",
    "\n",
    "1. <a href=\"#step1\">Check GPU memory</a>\n",
    "2. <a href=\"#step2\">Import libraries</a>\n",
    "3. <a href=\"#step3\">Prepare the training dataset</a>\n",
    "4. <a href=\"#step4\">Load a pre-trained LLM</a>\n",
    "5. <a href=\"#step5\">Define the trainer and fine-tune the LLM</a>\n",
    "6. <a href=\"#step6\">Deploy the fine-tuned model</a>\n",
    "7. <a href=\"#step7\">Test the deployed inference</a>\n",
    "\n",
    "\n",
    "NOTE: Work from the top to the bottom of this notebook, and do not skip sections; otherwise, you might receive error messages from missing code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6882540e-68c5-4536-b5b8-90c22439b0df",
   "metadata": {},
   "source": [
    "## <a name=\"step1\">Step 1: Check GPU memory</a>\n",
    "\n",
    "To check the GPU memory, run the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "635f5377-d55b-45d4-b9fd-92394eaa2b7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct 27 14:25:52 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       On  |   00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   23C    P8             11W /   70W |       1MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f226fdc-8694-4581-b8fd-fdf21a7d1589",
   "metadata": {},
   "source": [
    "If your CUDA memory is occupied by more than half (as in the following image), you need to shut down other running notebooks.\n",
    "\n",
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"images/memory.png\" alt=\"drawing\" width=\"800\"/> <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafdbd52-d0b1-428e-8955-8da5370ccdbd",
   "metadata": {},
   "source": [
    "## <a name=\"step2\">Step 2: Import libraries</a>\n",
    "\n",
    "Run the following two code blocks sequentially, one at a time, to import the necessary libraries, including the Hugging Face Transformers library and the PyTorch library, which is a dependency for Transformers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9473ffd8-877d-4d08-bb4f-f446bd95f126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install sagemaker --quiet --upgrade --force-reinstall\n",
    "!pip3 install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05425028-4ec9-4abd-8ec1-a94395cc8678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Any, Dict, List, Tuple, Union\n",
    "from datasets import Dataset, load_dataset, disable_caching\n",
    "disable_caching() ## disable huggingface cache\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TextDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846ab9fd-3095-4870-9001-71767da81d81",
   "metadata": {},
   "source": [
    "## <a name=\"step3\">Step 3: Prepare the training dataset</a>\n",
    "\n",
    "Load and view the dataset. For this practice lab, you use [Amazon SageMaker FAQs](https://aws.amazon.com/sagemaker/faqs/) for the main dataset, which has two columns: `instruction` and `response`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19a352ed-3da7-4e8d-8981-5f9b23478215",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed72616af274b839da0283cca49f841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'response'],\n",
       "    num_rows: 154\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_faqs_dataset = load_dataset(\"csv\", \n",
    "                                      data_files='data/amazon_sagemaker_faqs.csv')['train']\n",
    "sagemaker_faqs_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "518fc5c4-e448-4e55-9379-bddbbdcc7fc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'What is Amazon SageMaker?',\n",
       " 'response': 'Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_faqs_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fcb4ed-346e-42a1-ae66-2a0fd5b57a2b",
   "metadata": {},
   "source": [
    "### <a name=\"step3\">Step 3.1: Prepare the prompt</a>\n",
    "To fine-tune the LLM, you must decorate the instruction dataset with a PROMPT, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4755f14f-61da-483f-91ce-748fa968bbef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
       "            ### Instruction:\n",
       "            {instruction}\n",
       "            ### Response:\n",
       "            {response}\n",
       "            ### End"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.helpers import INTRO_BLURB, INSTRUCTION_KEY, RESPONSE_KEY, END_KEY, RESPONSE_KEY_NL, DEFAULT_SEED, PROMPT\n",
    "'''\n",
    "PROMPT = \"\"\"{intro}\n",
    "            {instruction_key}\n",
    "            {instruction}\n",
    "            {response_key}\n",
    "            {response}\n",
    "            {end_key}\"\"\"\n",
    "'''\n",
    "Markdown(PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e960228c-446a-429a-b6a1-473f6965293b",
   "metadata": {},
   "source": [
    "Now, feed the PROMPT to the dataset through the `_add_text` Python function, which takes a record as input. The function checks that both fields (instruction/response) are not null, and then passes the values to the predefined PROMPT template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "013cbcde-2237-4dff-ab77-9b1b9cb10d49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _add_text(rec):\n",
    "    instruction = rec[\"instruction\"]\n",
    "    response = rec[\"response\"]\n",
    "\n",
    "    if not instruction:\n",
    "        raise ValueError(f\"Expected an instruction in: {rec}\")\n",
    "\n",
    "    if not response:\n",
    "        raise ValueError(f\"Expected a response in: {rec}\")\n",
    "\n",
    "    rec[\"text\"] = PROMPT.format(\n",
    "        instruction=instruction, response=response)\n",
    "\n",
    "    return rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df0aea04-6e06-4879-8eb9-c1fb16483fcc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58215df44897484d849d7c764b2eb4ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'instruction': 'What is Amazon SageMaker?',\n",
       " 'response': 'Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.',\n",
       " 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n            ### Instruction:\\n            What is Amazon SageMaker?\\n            ### Response:\\n            Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\\n            ### End'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_faqs_dataset = sagemaker_faqs_dataset.map(_add_text)\n",
    "sagemaker_faqs_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf13a4cd-e013-4e68-bbf2-2aebc1d5a133",
   "metadata": {},
   "source": [
    "Use `Markdown` to neatly display the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "945c4f1d-2e3f-4b45-bec7-1f03612b1552",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
       "            ### Instruction:\n",
       "            What is Amazon SageMaker?\n",
       "            ### Response:\n",
       "            Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n",
       "            ### End"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(sagemaker_faqs_dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d83c07-93ae-48ea-a611-a6df6b44965c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <a name=\"#step4\">Step 4: Load a pre-trained LLM</a>\n",
    "\n",
    "\n",
    "To load a pre-trained model, initialize a tokenizer and a base model by using the `databricks/dolly-v2-3b` model from the Hugging Face Transformers library. The tokenizer converts raw text into tokens, and the base model generates text based on a given prompt. By following the instructions previously outlined, you can correctly instantiate these components and use their functionality in your code.\n",
    "\n",
    "\n",
    "The `AutoTokenizer.from_pretrained()` Python function is used to instantiate the tokenizer. \n",
    "- `padding_side=\"left\"` specifies the side of the sequences where padding tokens are added. In this case, padding tokens are added to the left side of each sequence. \n",
    "- `eos_token` is a special token that represents the end of a sequence. By assigning the token to `pad_token`, any padding tokens added during tokenization are also considered end-of-sequence tokens. This can be useful when generating text through the model because the model knows when to stop generating text after encountering padding tokens.\n",
    "- `tokenizer.add_special_tokens...` adds three additional special tokens to the tokenizer's vocabulary. These tokens likely serve specific purposes in the application using the tokenizer. For example, the tokens could be used to mark the end of an input, an instruction, or a response in a dialogue system.\n",
    "\n",
    "After the function runs, the `tokenizer` object has been initialized and is ready to use for tokenizing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dbd90bc-74f8-4d91-b70d-81f469483958",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14243c51fffc4fe2baed56c06ab3d0c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/450 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01cadf6cb5dd457588e93a39fcf3f3e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cfa9051376743afb685c0ddf6f18c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-3b\", \n",
    "                                          padding_side=\"left\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": \n",
    "                              [END_KEY, INSTRUCTION_KEY, RESPONSE_KEY_NL]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcd4d817-2008-42c2-84d5-13867c8053cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f768841db5b24a1a89cd49e0feb0bb2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/819 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045f6f6d61804577a63b162edd3b3970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/5.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"databricks/dolly-v2-3b\",\n",
    "    # use_cache=False,\n",
    "    device_map=\"auto\", #\"balanced\",\n",
    "    load_in_8bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4d9354-c68b-4637-8985-8f781fadfae0",
   "metadata": {},
   "source": [
    "### <a name=\"#step4.1\">Step 4.1: Prepare the model for training</a>\n",
    "Some preprocessing is needed before training an INT8 model using Parameter-Efficient Fine-Tuning (PEFT); therefore, import a utility function, `prepare_model_for_int8_training`, that will:\n",
    "\n",
    "- Cast all the non-INT8 modules to full precision (FP32) for stability.\n",
    "- Add a forward_hook to the input embedding layer to enable gradient computation of the input hidden states.\n",
    "- Enable gradient checkpointing for more memory-efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df096cfc-4c37-4155-bb77-e2c7164e3bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50281, 2560)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cc7ea0-89b3-4164-a8d5-3ced6179645f",
   "metadata": {},
   "source": [
    "Use the `preprocess_batch` function to preprocess the text field of the batch, applying tokenization, truncation, and other relevant operations based on the specified maximum length. The field takes a batch of data, a tokenizer, and a maximum length as input. \n",
    "\n",
    "For more details, refer to `mlu_utils/helpers.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9de28ac-e6cb-46f3-8410-8d82923b1270",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from utils.helpers import mlu_preprocess_batch\n",
    "\n",
    "MAX_LENGTH = 256\n",
    "_preprocessing_function = partial(mlu_preprocess_batch, max_length=MAX_LENGTH, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04a47b1-03c4-45b9-82b6-14bbf3c46bd0",
   "metadata": {},
   "source": [
    "Next, apply the preprocessing function to each batch in the dataset, modifying the text field accordingly. The map operation is performed in a batched manner and the instruction, response, and text columns are removed from the dataset. Finally, `processed_dataset` is created by filtering `sagemaker_faqs_dataset` based on the length of the input_ids field, ensuring that it fits within the specified `MAX_LENGTH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae5cd938-efa0-4fe7-b24d-a5e2cdf3cf9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d48bc1aacf9b4da0977c1e197ecfd7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79cea82377cc4deab80d18387eef0bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_sagemaker_faqs_dataset = sagemaker_faqs_dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"instruction\", \"response\", \"text\"],\n",
    ")\n",
    "\n",
    "processed_dataset = encoded_sagemaker_faqs_dataset.filter(lambda rec: len(rec[\"input_ids\"]) < MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd86b08-e90e-4fad-9107-f9dcf63915ad",
   "metadata": {},
   "source": [
    "Split the dataset into `train` and `test` for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db5ba6bb-41a7-455d-a349-8219405eec98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 133\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 14\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset = processed_dataset.train_test_split(test_size=14, seed=0)\n",
    "split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bea1a4a-3a36-4bce-a924-dc876ab91a78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 154\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sagemaker_faqs_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96736250-315b-4465-a3f6-e6c05cc242b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 147\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b2da6-08d9-4a1d-bfa9-0e847e104ce9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <a name=\"#step5\">Step 5: Define the trainer and fine-tune the LLM</a>\n",
    "\n",
    "To efficiently fine-tune a model, in this practice lab, you use [LoRA: Low-Rank Adaptation](https://arxiv.org/abs/2106.09685). LoRA freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and reduce the GPU memory requirement by 3 times. \n",
    "\n",
    "\n",
    "### <a name=\"#step5.1\">Step 5.1: Define LoraConfig and load the LoRA model</a> \n",
    "\n",
    "Use the build LoRA class `LoraConfig` from [huggingface ðŸ¤— PEFT: State-of-the-art Parameter-Efficient Fine-Tuning](https://github.com/huggingface/peft). Within `LoraConfig`, specify the following parameters:\n",
    "\n",
    "- `r`, the dimension of the low-rank matrices\n",
    "- `lora_alpha`, the scaling factor for the low-rank matrices\n",
    "- `lora_dropout`, the dropout probability of the LoRA layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d98cace8-19f6-41c0-9fa2-04b8bb987fe0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
    "\n",
    "MICRO_BATCH_SIZE = 8  \n",
    "BATCH_SIZE = 64\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "LORA_R = 256 # 512\n",
    "LORA_ALPHA = 512 # 1024\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "                 r=LORA_R,\n",
    "                 lora_alpha=LORA_ALPHA,\n",
    "                 lora_dropout=LORA_DROPOUT,\n",
    "                 bias=\"none\",\n",
    "                 task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6166278b-9810-4d25-a5b2-95db0e376d76",
   "metadata": {},
   "source": [
    "Use the `get_peft_model` function to initialize the model with the LoRA framework, configuring it based on the provided `lora_config` settings. This way, the model can incorporate the benefits and capabilities of the LoRA optimization approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6042bbe6-6815-4f19-95c1-c8946f6156d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 83886080 || all params: 2858977280 || trainable%: 2.9341289483769524\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f590f0f-e95e-4407-8716-e0a802ccc2b9",
   "metadata": {},
   "source": [
    "As shown, LoRA-only trainable parameters are about 3 percent of the full weights, which is much more efficient.\n",
    "\n",
    "### <a name=\"#step5.2\">Step 5.2: Define the data collator</a>\n",
    "\n",
    "A DataCollator is a huggingfaceðŸ¤— transformers function that takes a list of samples from a dataset and collates them into a batch, as a dictionary of PyTorch tensors.\n",
    "\n",
    "Use `DataCollatorForCompletionOnlyLM`, which extends the functionality of the base `DataCollatorForLanguageModeling` class from the Transformers library. This custom collator is designed to handle examples where a prompt is followed by a response in the input text and the labels are modified accordingly.\n",
    "\n",
    "For implementation, refer to `utils/helpers.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e5cef99-9c84-4478-be05-317b7f79de45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.helpers import MLUDataCollatorForCompletionOnlyLM\n",
    "\n",
    "data_collator = MLUDataCollatorForCompletionOnlyLM(\n",
    "        tokenizer=tokenizer, mlm=False, return_tensors=\"pt\", pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9321ebc5-c2db-4c44-9442-961122da9205",
   "metadata": {},
   "source": [
    "### <a name=\"#step5.3\">Step 5.3: Define the trainer</a>\n",
    "\n",
    "To fine-tune the LLM, you must define a trainer. First, define the training arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07571a85-2724-44ee-a2b0-24ed5866ce2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4  \n",
    "MODEL_SAVE_FOLDER_NAME = \"dolly-3b-lora\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "                    output_dir=MODEL_SAVE_FOLDER_NAME,\n",
    "                    fp16=True,\n",
    "                    per_device_train_batch_size=1,\n",
    "                    per_device_eval_batch_size=1,\n",
    "                    learning_rate=LEARNING_RATE,\n",
    "                    num_train_epochs=EPOCHS,\n",
    "                    logging_strategy=\"steps\",\n",
    "                    logging_steps=100,\n",
    "                    evaluation_strategy=\"steps\",\n",
    "                    eval_steps=100, \n",
    "                    save_strategy=\"steps\",\n",
    "                    save_steps=20000,\n",
    "                    save_total_limit=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1b13b8-0139-48f5-8619-acf459b78d33",
   "metadata": {},
   "source": [
    "This is where the magic happens! Initialize the trainer with the defined model, tokenizer, training arguments, data collator, and the train/eval datasets. \n",
    "\n",
    "The training takes about 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a708616f-8fbc-4f1e-9f50-077ae198fd69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:219: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:413.)\n",
      "  attn_scores = torch.where(causal_mask, attn_scores, mask_value)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1330' max='1330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1330/1330 14:25, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.146100</td>\n",
       "      <td>2.089690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.597900</td>\n",
       "      <td>2.163289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.263900</td>\n",
       "      <td>2.322221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.851900</td>\n",
       "      <td>2.315344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.467100</td>\n",
       "      <td>2.589732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.322500</td>\n",
       "      <td>2.867051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.202100</td>\n",
       "      <td>2.887177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.114300</td>\n",
       "      <td>3.069107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.052300</td>\n",
       "      <td>3.214638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.036700</td>\n",
       "      <td>3.303431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>3.432382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>3.445980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>3.500467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1330, training_loss=0.5334119112531942, metrics={'train_runtime': 868.0445, 'train_samples_per_second': 1.532, 'train_steps_per_second': 1.532, 'total_flos': 2710163221708800.0, 'train_loss': 0.5334119112531942, 'epoch': 10.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=split_dataset['train'],\n",
    "        eval_dataset=split_dataset[\"test\"],\n",
    "        data_collator=data_collator,\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedf5ffa-3997-415f-bf51-dd91d8c3d777",
   "metadata": {},
   "source": [
    "### <a name=\"#step5.4\">Step 5.4: Save the fine-tuned model</a>\n",
    "\n",
    "\n",
    "When the training is completed, you can save the model to a directory by using the [`transformers.PreTrainedModel.save_pretrained`] function. \n",
    "This function saves only the incremental ðŸ¤— PEFT weights (adapter_model.bin) that were trained, so the model is very efficient to store, transfer, and load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97f327e9-c6cd-4f2b-af25-e22420372e6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(MODEL_SAVE_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a9cc6d-98cf-4118-8218-6de6c9893570",
   "metadata": {},
   "source": [
    "If you want to save the full model that you just fine-tuned, you can use the [`transformers.trainer.save_model`] function. Meanwhile, save the training arguments together with the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a112abbd-8b1b-4da7-80d4-35b92e4eadcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "198216a6-598e-4d56-af8d-682a71b187bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.model.config.save_pretrained(MODEL_SAVE_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b33f0f9-2604-45fb-a32d-6a1b2e311513",
   "metadata": {},
   "source": [
    "Save the tokenizer along with the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56aac92b-8bcb-46dc-87d6-ff5069b0896c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('dolly-3b-lora/tokenizer_config.json',\n",
       " 'dolly-3b-lora/special_tokens_map.json',\n",
       " 'dolly-3b-lora/tokenizer.json')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(MODEL_SAVE_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d3c515-1c98-4282-9040-7af7d702fb8e",
   "metadata": {},
   "source": [
    "## <a name=\"#step6\">Step 6: Deploy the fine-tuned model</a>\n",
    "\n",
    "### <a name=\"step6\">Overview of deployment parameters</a>\n",
    "\n",
    "To deploy using the Amazon SageMaker Python SDK with the Deep Java Library (DJL), you must instantiate the `Model` class with the following parameters:\n",
    "```{python}\n",
    "model = Model(\n",
    "    image_uri,\n",
    "    model_data=...,\n",
    "    predictor_cls=...,\n",
    "    role=aws_role\n",
    ")\n",
    "```\n",
    "- `image_uri`: The Docker image URI representing the deep learning framework and version to be used.\n",
    "- `model_data`: The location of the fine-tuned LLM model artifact in an Amazon Simple Storage Service (Amazon S3) bucket. It specifies the path to the TAR GZ file containing the model's parameters, architecture, and any necessary artifacts.\n",
    "- `predictor_cls`: This is just a JSON in JSON out predictor, nothing DJL related. For more information, see [sagemaker.djl_inference.DJLPredictor](https://sagemaker.readthedocs.io/en/stable/frameworks/djl/sagemaker.djl_inference.html#djlpredictor).\n",
    "- `role`: The AWS Identity and Access Management (IAM) role ARN that provides necessary permissions to access resources, such as the S3 bucket that contains the model data.\n",
    "\n",
    "### <a name=\"step6.1\">Step 6.1: Instantiate SageMaker parameters</a>\n",
    "\n",
    "Initialize an Amazon SageMaker session and retrieve information related to the AWS environment such as the SageMaker role and AWS Region. You also specify the image URI for a specific version of the \"djl-deepspeed\" framework by using the SageMaker session's Region. The image URI is a unique identifier for a specific Docker container image that can be used in various AWS services, such as Amazon SageMaker or Amazon Elastic Container Registry (Amazon ECR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55f11d90-e6ac-42f4-a8ef-b74f55d8c15d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker_session:  <sagemaker.session.Session object at 0x7fb37c0b6170>\n",
      "aws_role:  arn:aws:iam::976066288830:role/sagemaker_notebook_role\n",
      "aws_region:  us-east-1\n",
      "image_uri:  763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.22.1-deepspeed0.9.2-cu118\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker.djl_inference\n",
    "from sagemaker.session import Session\n",
    "from sagemaker import image_uris\n",
    "from sagemaker import Model\n",
    "\n",
    "sagemaker_session = Session()\n",
    "print(\"sagemaker_session: \", sagemaker_session)\n",
    "\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "print(\"aws_role: \", aws_role)\n",
    "\n",
    "aws_region = boto3.Session().region_name\n",
    "print(\"aws_region: \", aws_region)\n",
    "\n",
    "image_uri = image_uris.retrieve(framework=\"djl-deepspeed\",\n",
    "                                version=\"0.22.1\",\n",
    "                                region=sagemaker_session._region_name)\n",
    "print(\"image_uri: \", image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea3ae0e-6639-4400-9123-c38f1b79fd99",
   "metadata": {},
   "source": [
    "\n",
    "### <a name=\"step6.2\">Step 6.2: Create the model artifact</a> ###\n",
    "\n",
    "To upload the model artifact to the S3 bucket, we need to create a TAR GZ file that contains the model's parameters. First, create a directory named `lora_model` and a subdirectory named `dolly-3b-lora`. The \"-p\" option ensures that the command creates any intermediate directories if they don't exist. Then, copy the LoRA checkpoints, `adapter_model.bin` and `adapter_config.json`, to `dolly-3b-lora`. The base Dolly model is downloaded at runtime from the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5685e182-95e0-4ec3-888d-bb573ebdd3db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf lora_model\n",
    "mkdir -p lora_model\n",
    "mkdir -p lora_model/dolly-3b-lora\n",
    "cp dolly-3b-lora/adapter_config.json lora_model/dolly-3b-lora/\n",
    "cp dolly-3b-lora/adapter_model.bin lora_model/dolly-3b-lora/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df3e690-c244-44b6-9ff7-7a51d2ee47e3",
   "metadata": {},
   "source": [
    "Next, set the [DJL Serving configuration options](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-configuration.html) in `serving.properties`. Using the Jupyter `%%writefile` magic command, you can write the following content to a file named lora_model/serving.properties.\n",
    "- `engine=Python`: This line specifies the engine used for serving.\n",
    "- `option.entryPoint=model.py`: This line specifies the entry point for the serving process, which is set to model.py. \n",
    "- `option.adapter_checkpoint=dolly-3b-lora`: This line sets the checkpoint for the adapter to dolly-3b-lora. A checkpoint typically represents the saved state of a model or its parameters.\n",
    "- `option.adapter_name=dolly-lora`: This line sets the name of the adapter to dolly-lora, a component that helps interface between the model and the serving infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34fa2845-a04b-48aa-8381-a8097bce07b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing lora_model/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile lora_model/serving.properties\n",
    "engine=Python\n",
    "option.entryPoint=model.py\n",
    "option.adapter_checkpoint=dolly-3b-lora\n",
    "option.adapter_name=dolly-lora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91554e8a-c65d-409b-bf16-29c179a7a6df",
   "metadata": {},
   "source": [
    "You also need the environment requirement file in the model artifact. Create a file named `lora_model/requirements.txt` and write a list of Python package requirements, typically used with package managers such as `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c76b09e8-3080-4f79-bec8-7ebfb2feeb64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing lora_model/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile lora_model/requirements.txt\n",
    "accelerate>=0.16.0,<1\n",
    "bitsandbytes==0.39.0\n",
    "click>=8.0.4,<9\n",
    "datasets>=2.10.0,<3\n",
    "deepspeed>=0.8.3,<0.9\n",
    "faiss-cpu==1.7.4\n",
    "ipykernel==6.22.0\n",
    "scipy==1.11.1\n",
    "torch>=2.0.0\n",
    "transformers==4.28.1\n",
    "peft==0.3.0\n",
    "pytest==7.3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca79001-7dd7-49ca-80e1-3ee88a8034b8",
   "metadata": {},
   "source": [
    "### <a name=\"step6.3\">Step 6.3: Create the inference script</a>\n",
    "\n",
    "Similar to the fine-tuning notebook, a custom pipeline, `InstructionTextGenerationPipeline`, is defined. The code is provided in `utils/deployment_model.py`. \n",
    "\n",
    "You save these inference functions to `lora_model/model.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8be0229e-0895-436e-b789-edbb3ba773cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp utils/deployment_model.py lora_model/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329d7b1f-7b8d-444a-b185-a46053421536",
   "metadata": {},
   "source": [
    "### <a name=\"step6.4\">Step 6.4: Upload the model artifact to Amazon S3</a>\n",
    "\n",
    "Create a compressed tarball archive of the lora_model directory and save it as lora_model.tar.gz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86c84297-6f89-47b5-9f11-2cf1d2fba989",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lora_model/\n",
      "lora_model/requirements.txt\n",
      "lora_model/dolly-3b-lora/\n",
      "lora_model/dolly-3b-lora/adapter_model.bin\n",
      "lora_model/dolly-3b-lora/adapter_config.json\n",
      "lora_model/serving.properties\n",
      "lora_model/model.py\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tar -cvzf lora_model.tar.gz lora_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9e15a5-832a-453c-b02d-b0e5c82ca8b5",
   "metadata": {},
   "source": [
    "Upload the lora_model.tar.gz file to the specified S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "383a3067-47f1-4ddc-9f81-0d6c33801d4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifact-f32ddcf0\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker.djl_inference\n",
    "from sagemaker.session import Session\n",
    "from sagemaker import image_uris\n",
    "from sagemaker import Model\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Get the name of the bucket with prefix lab-code\n",
    "for bucket in s3.buckets.all():\n",
    "    if bucket.name.startswith('artifact'):\n",
    "        mybucket = bucket.name\n",
    "        print(mybucket)\n",
    "    \n",
    "response = s3_client.upload_file(\"lora_model.tar.gz\", mybucket, \"lora_model.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d142b4-c420-416e-a0c5-3068a934d573",
   "metadata": {},
   "source": [
    "### <a name=\"step6.5\">Step 6.5: Deploy the model</a> ###\n",
    "\n",
    "Now, it's the time to deploy the fine-tuned LLM by using the SageMaker Python SDK. The SageMaker Python SDK `Model` class is instantiated with the following parameters:\n",
    "\n",
    "- `image_uri`: The Docker image URI that represents the deep learning framework and version to be used.\n",
    "- `model_data`: The location of the fine-tuned LLM model artifact in an S3 bucket. It specifies the path to the TAR GZ file that contains the model's parameters, architecture, and any necessary artifacts.\n",
    "- `predictor_cls`: This is just a JSON in JSON out predictor, nothing DJL related. For more information, see [sagemaker.djl_inference.DJLPredictor](https://sagemaker.readthedocs.io/en/stable/frameworks/djl/sagemaker.djl_inference.html#djlpredictor).\n",
    "- `role`: The IAM role ARN that provides necessary permissions to access resources, such as the S3 bucket that contains the model data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c7f0b12-bf0f-4fd8-bb7d-a0000ee67b1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_data=\"s3://{}/lora_model.tar.gz\".format(mybucket)\n",
    "\n",
    "model = Model(image_uri=image_uri,\n",
    "              model_data=model_data,\n",
    "              predictor_cls=sagemaker.djl_inference.DJLPredictor,\n",
    "              role=aws_role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946d5263-2c04-4541-8126-e718ab7e55ce",
   "metadata": {},
   "source": [
    "NOTE: The deployment should be completed within 10 minutes. Any longer than that, your endpoint might have failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "269abf73-c194-48c0-8f5e-6c23547acb5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------!CPU times: user 406 ms, sys: 98.6 ms, total: 505 ms\n",
      "Wall time: 7min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictor = model.deploy(1, \"ml.g4dn.2xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db6aff0-5525-4bc8-945b-96391b9fc1bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <a name=\"step7\">Step 7: Test the deployed inference</a>\n",
    "\n",
    "Test the inference endpoint with [predictor.predict](https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html#sagemaker.predictor.Predictor.predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "20d371af-3486-441f-8d6f-ef65bf12395d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = predictor.predict({\"inputs\": \"What solutions come pre-built with Amazon SageMaker JumpStart?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b2df51a6-6e71-4c3a-b2be-116fc07851a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "SageMaker JumpStart includes solutions that are preconfigured with all necessary AWS services to launch a solution into production. Solutions are fully customizable so you can easily modify them to fit your specific use case and dataset. You can use solutions for over 15 use cases including demand forecasting, fraud detection, and predictive maintenance, and readily deploy solutions with just a few clicks. For more information about all solutions available, visit the SageMakerÂ getting started page."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f81c0a4-71de-41b3-b759-3fffe7cd469c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
